{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/vs265header.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align=\"center\">Lab 3 - Unsupervised Learning </h1>\n",
    "<h2 align=\"center\"> Part 1 - Simple Datasets </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### *** NOTE *** ###\n",
    "### This lab uses alot of animation so you may want to open your notebook \n",
    "### with 'jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000'\n",
    "### so it doesn't throw warnings constantly.\n",
    "### *** NOTE *** ###\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import utils.lab3utils as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hebbian Learning as Neural PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `data/data2d.npz` contains two arrays of data that will be used for this problem, $D_1$ and $D_2$, each of which contains 1000 data points in two dimensions. We load it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the D1 data from data2d.npz\n",
    "D1 = np.load('./data/data2d.npz')['D1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconstrained Hebbian Learning\n",
    "\n",
    "Train a single linear neuron on this data using Hebbian learning (unconstrained). Plot the weight vector along with the data on each weight update.\n",
    "\n",
    "The data-loading and results-plotting code has been provided for you. You just need to add the missing lines to the function `hebbLearn` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hebbLearn(dataset, weights, learningRate):\n",
    "    \"\"\"\n",
    "    Weight update with a Hebbian rule.\n",
    "    weights should be provided by output of util.initialize()\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset      : dataset, numpy array, either D1 or D2, shape=(2, 1000)\n",
    "    weights      : numpy array, weight matrix of linear transformation of input \n",
    "                   data, shape=(2, N) where N is the number of neurons\n",
    "    learningRate : float, scaling factor for gradients\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weights      : numpy array, Hebbian-updated weight matrix \n",
    "                   of linear transformation of input data\n",
    "    \"\"\"\n",
    "    \n",
    "    _, numDatapoints = dataset.shape\n",
    "    batchLearningRate = learningRate/numDatapoints\n",
    "    \n",
    "    output = # YOUR CODE - compute neuron output for all data (can be done as one line)\n",
    "    \n",
    "    dw = # YOUR CODE - compute dw: Hebbian learning rule \n",
    "\n",
    "    weights += # YOUR CODE - update weight vector by dw using batchLearningRate\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set the hyperparameters\n",
    "numTrials = 250\n",
    "learningRate = 0.01\n",
    "\n",
    "# Now we initialize the run\n",
    "figure, plottedWeightVector, weights = util.initialize(D1)\n",
    "\n",
    "# and then iterate on weight updates\n",
    "weights = util.doLearn(hebbLearn, D1, figure, plottedWeightVector, weights, learningRate, numTrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oja's Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply Oja’s single-neuron learning rule to constrain the growth of\n",
    "the weight vector, and again show how the weight vector evolves during\n",
    "learning. As before, you only need to add a few lines to the `ojaLearn` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ojaLearn(dataset,weights,learningRate):\n",
    "    \"\"\"\n",
    "    Weight update with the Oja rule.\n",
    "    weights should be provided by output of util.initialize()\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset      : dataset, numpy array, either D1 or D2, shape=(2, 1000)\n",
    "    weights      : numpy array, weight matrix of linear transformation of input \n",
    "                   data, shape=(2, N) where N is the number of neurons\n",
    "    learningRate : float, scaling factor for gradients\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights      : numpy array, Oja-updated weight matrix \n",
    "                   of linear transformation of input data\n",
    "    \"\"\"\n",
    "    \n",
    "    _, numDatapoints = dataset.shape\n",
    "    batchLearningRate = learningRate/numDatapoints\n",
    "    \n",
    "    output = # YOUR CODE: compute neuron output for all data\n",
    "    \n",
    "    dw = # YOUR CODE: compute dw: Oja learning rule \n",
    "\n",
    "    weights += #YOUR CODE: update weight vector by dw using batchLearningRate\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we set the hyperparameters\n",
    "numTrials = 250\n",
    "learningRate = 0.01\n",
    "\n",
    "# now we initialize the run\n",
    "figure, plottedWeightVector, weights = util.initialize(D1)\n",
    "\n",
    "# and then iterate\n",
    "weights = util.doLearn(ojaLearn, D1, figure, plottedWeightVector, weights, learningRate, numTrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is different about the weight vector learned by Oja's Rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sanger's Rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Sanger’s rule to train two neurons to represent the principal components\n",
    "of the data.  Make sure you run the algorithm for long enough (>1000 steps)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sangerLearn(dataset,weights,learningRate):\n",
    "    \"\"\"\n",
    "    Weight update with the Sanger rule.\n",
    "    weights and learningRate should be provided by output of util.initialize()\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset      : dataset, numpy array, either D1 or D2, shape=(2, 1000)\n",
    "    weights      : numpy array, weight matrix of linear transformation of input \n",
    "                   data, shape=(2, N) where N is the number of neurons\n",
    "    learningRate : float, scaling factor for gradients\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weights      : numpy array, Sanger-updated weight matrix \n",
    "                   of linear transformation of input data\n",
    "    \"\"\"\n",
    "    _, numDatapoints = dataset.shape\n",
    "    batchLearningRate = learningRate/numDatapoints\n",
    "    \n",
    "    output = #YOUR CODE: compute neuron output for all data\n",
    "    numOutputs = output.shape[0]\n",
    "    \n",
    "    residual = dataset\n",
    "    dw = np.zeros(weights.shape)\n",
    "    \n",
    "    for i in range(numOutputs):\n",
    "        \n",
    "        residual = residual - #YOUR CODE: what component of the input \n",
    "                            #is explained by neurons before neuron i?\n",
    "        \n",
    "        dw[:,i] = # YOUR CODE: use the residual to update weights for neuron i\n",
    "\n",
    "    weights += # YOUR CODE: update weight vector by dw using batchLearningRate\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first we set the hyperparameters\n",
    "numTrials = 1000\n",
    "learningRate = 0.1\n",
    "\n",
    "# now we initialize the run and view the data\n",
    "figure, plottedWeightVectors, weights = util.initialize(D1,numOutputs=2)\n",
    "\n",
    "# and then iterate\n",
    "weights = util.doLearn(sangerLearn, D1, figure, plottedWeightVectors, weights, learningRate, numTrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the angle between the two weight vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Gaussian Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cells below to load the dataset `D2` and learn weight vectors using the vanilla Hebb's rule, Oja's rule, and Sanger's rule (with two components) code that you wrote. \n",
    "\n",
    "What's different about the results from this dataset? Can you explain why there's a difference? \n",
    "\n",
    "Don't worry if your Sanger-trained network doesn't converge well, so long as it works on the first dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we load the Non-Gaussian data, D2, from data2d.npz\n",
    "D2 = np.load('./data/data2d.npz')['D2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrials = 250\n",
    "learningRate = 0.1\n",
    "\n",
    "figure, plottedWeightVector, weights = util.initialize(D2)\n",
    "\n",
    "weights = util.doLearn(hebbLearn, D2, figure, plottedWeightVector, weights, learningRate, numTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrials = 100\n",
    "learningRate = 0.05\n",
    "\n",
    "figure, plottedWeightVector, weights = util.initialize(D2)\n",
    "\n",
    "weights = util.doLearn(ojaLearn, D2, figure, plottedWeightVector, weights, learningRate, numTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrials = 250\n",
    "learningRate = 0.25\n",
    "\n",
    "figure, plottedWeightVector, weights = util.initialize(D2, numOutputs=2)\n",
    "weights = util.doLearn(sangerLearn, D2, figure, plottedWeightVector, weights, learningRate, numTrials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winner-Take-All Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train a 4-unit Winner-Take-All network using the standard competitive learning rule as in equation 9.7 from Hertz, Krogh, and Palmer. *HINT:* the numpy function `argmax` will be helpful for figuring out the winner for each input.\n",
    "\n",
    "Don't worry if it takes multiple runs to get your algorithm to converge to the correct solution. Unlike Hebbian learning and other PCA methods, WTA learning is finicky -- the results are highly dependent on the initialization, which is typically random. If you want to make sure that your algorithm is coded correctly, pass `goodWeights=True` to the function `initializeWTA` below, which gives a non-random initialization that will work if you've coded WTA correctly. **NOTE** Your method may work with goodWeights=True even it isn't correct, please keep this in mind.\n",
    "\n",
    "Once you've got the basic algorithm working, implement one (or more) of the strategies suggested on p221 of Hertz, Krogh, and Palmer to improve convergence behavior and reduce \"dead units\". A common choice is \"leaky learning\" -- update all neurons on every trial, but update winners with a learning rate that is at least an order of magnitude less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WTALearn(dataset,weights,learningRate):\n",
    "    \"\"\"\n",
    "    Weight update with the WTA rule.\n",
    "    weights and learningRate should be provided by output of initializeWTA()\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset      : dataset, numpy array, either D1 or D2, shape=(2, 1000)\n",
    "    weights      : numpy array, weight matrix of linear transformation of input \n",
    "                   data, shape=(2, N) where N is the number of neurons\n",
    "    learningRate : float, factor to multiply weight updates\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weights      : numpy array, Sanger-updated weight matrix \n",
    "                                 of linear transformation of input data\n",
    "    \"\"\"\n",
    "    \n",
    "    _, numDatapoints = dataset.shape\n",
    "    batchLearningRate = learningRate/numDatapoints\n",
    "    \n",
    "    output = # YOUR CODE: compute neuron output for all data\n",
    "    winnerIndices =  #YOUR CODE: try np.argmax\n",
    "    \n",
    "    numOutputs = output.shape[0]\n",
    "    \n",
    "    dw = np.zeros(weights.shape)\n",
    "    \n",
    "    for i in range(numOutputs):\n",
    "        winnerMask = #YOUR CODE: make a binary vector numDatapoints long that is 1 wherever neuron i was the \"winner\"\n",
    "        \n",
    "        dw[:, i, None] = # YOUR CODE: update neuron i to move it towards stimuli on which it \"won\"\n",
    "    \n",
    "    weights += #YOUR CODE: update weights by dw using batchLearningRate\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrials = 150\n",
    "learningRate = 0.5\n",
    "\n",
    "# pass goodWeights=True to use a non-random initialization\n",
    "# that will converge if your algorithm is correctly implemented\n",
    "\n",
    "figure, plottedWeightVectors, weights = util.initializeWTA(\n",
    "                                          D2, goodWeights=False, numOutputs=4)\n",
    "\n",
    "weights = util.doLearn(WTALearn, D2, figure, plottedWeightVectors, weights, learningRate, numTrials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
